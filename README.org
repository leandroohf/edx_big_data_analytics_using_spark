-*- Mode: org; mode: auto-fill; fill-column: 76; org-download-image-dir: "./img/";buffer-auto-save-file-name: nil;epa-file-encrypt-to: nil -*-

* Big Data Analytics Using Spark

  *UC San Diego*

  *Yoav Freund*
  Professor of Computer Science and Engineering
  UC San Diego

  https://www.edx.org/course/big-data-analytics-using-spark-uc-san-diegox-dse230x
  
** About

   https://courses.edx.org/courses/course-v1:UCSanDiegoX+DSE230x+1T2019/course/

   In data science, data is called “big” (called big and not Big data) if it
   cannot fit into the memory of a single standard laptop or workstation.

   The analysis of big datasets requires using a cluster of tens, hundreds or
   thousands of computers. Effectively using such clusters requires the use of
   distributed files systems, such as the Hadoop Distributed File System (HDFS)
   and corresponding computational models, such as Hadoop, MapReduce and Spark.

   In this course, part of the Data Science MicroMasters program, you will learn
   what the bottlenecks are in massive parallel computation and how to use spark
   to minimize these bottlenecks.

   You will learn how to perform supervised an unsupervised machine learning on
   massive datasets using the Machine Learning Library (MLlib).

   In this course, as in the other ones in this MicroMasters program, you will
   gain hands-on experience using PySpark within the Jupyter notebooks
   environment.

** What you'll learn
   
   
   * Programming Spark using Pyspark
   * Identifying the computational tradeoffs in a Spark application
   * Performing data loading and cleaning using Spark and Parquet
   * Modeling data through statistical and machine learning methods

** [#A] Main learning's

   1. latency (IO operations) is the main bottleneck in big data
   2. Caching can minimize latency
   3. Map reduce operations can NOT be depended of the order of the data and
      the operations, because when running in parallel it is not possible
      the guarantee the order
   4. Hadoop properties 
      1. Redundancy: Data copied across the clusters node and data can be
         recovery if a node fails
      2. Locality: node process data that it is in his disk
      3. Data: Process in parallel
   5. 3 groups of commands
      1. Creation: RDD from files, databases, or data on the driver
      2. Transformations: transform RDD in another RDD
      3. Actions: RDD to list on driver node, databases or files


** Week 1 n 2
*** Latency

    #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-27 17:41:49
    [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-27_17-41-49.png]]


    1. The major latency in big data analysis is reading and writing to storage
    2. Different types of storage offer different latency
    3. Big data solutions revolve around methods to for organizing storage and
       computations in ways that maximize speed while minimizing cost
    4. Key point is,memory locality
      
    memory trade off

    #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-28 10:09:49
    [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-28_10-09-49.png]]

   
    When a CPU needs to a computation with a value (Ex: 67). It first check if it is
    in the cache. If it is the value can be retrieve quickly if not will take
    longer
   
    1. cache hit: value is memory
    2. cache missing
       1. free space in the cache for instance element 67
       2. write value in cache
       3. Read value in cache



    Access locality
   
    * The cache is effective if cache hit rate is high
    * temporally locality: program access the same memory address many time in a
      short period of time
    * spacial locality: access multiple memory address close to each other, like
      consecutive address (Ex: 50,51,52,53)
      * sorted data increase spacial locality
      * linked list (poor locality)
      * indexed array (good locality)


    #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-28 10:31:59
    [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-28_10-31-59.png]]



    #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-28 10:32:33
    [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-28_10-32-33.png]]

    Remember memory RAM are split in pages, and data in the same page is easy to
    be copy and transferred to the cache

    Summary

    * Caching reduces storage latency by bringing relevant data close to the CPU
    * This requires that CODE exhibits access locality. You need:
      * Temporal locality (How to guarantee that in code?)
      * Spatial locality (How to guarantee that in code?)


    Row-wise (numpy, matlab n R) vs Column-wise (IDL needs to double check)
    * Row-wise: scanning row by row is faster than column by column (due spatial
     locality)

     Code to test it's But I am not 100% convinced about that. For better
      measurment need to repeat n times
     #+begin_src python
       # This is very complicated. Because for me the matrix should be square

       n = 10
       a = np.array([range(1,n*n+1)]).reshape(n,n)
       a

       # XXX: If random did not work 
       # a = np.random.rand(n,n)
       # a

       %%timeit

       #Scan column by column

       s = 0
       for i in range(n): s+=sum(a[:,i])

       print(s)
       # 31.5 s ± 93.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

       %%time 
       #Scan row by row
       s= 0
       for i in range(n): s+=sum(a[i,:])

       print(s)
       # 31.2 s ± 149 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) !?
     #+end_src

    Conclusions:

    1. Traversing a numpy array column by column takes more than row by row
    2. The effect increase with the size of the array
    3. The effect is highly variable between runs due to state of the cache n
       process running on CPU
*** Measuring Latency

    This is the most important things that affects performance in system with
    large data.

    * L1 Cache latency is very slow
    * external memory like RAM or SSD are very slow

    How to measure

    The software that he uses is accurate to only to 1 ms (10^{-6}) and not to
    nano seconds.
    
    #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-29 09:17:42
    [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-29_09-17-42.png]]

     Some notes about how to visualize and analyze long tails distributions:
     * They have a high number of cases in large ranges that happens few times.
       So if you count all together is a significant number of cases, but each
       case happens few times. See the histograms visualization. Due to scale
       you will not be able to see the events in long tail range. So the
       histograms was split into 2 ranges. Another problem with histogram not
       related to long tails is to choose the bin size.

     #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-29 09:29:41
     [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-29_09-29-41.png]]
       
     * One solution,that is less intuitive is to see the probabilities of the
       event happens. Special cumulative density function CDF. But as you get
       used to it will get right intuitive
       * We do not have to deal with bin size
       * CDF(a) = Pr(T <= a) where T is latency
       * Instead of CDF we will plot the 1 - CDF(a) the probability of T be
         large than a and compare with Gaussian that is something that we are
         familiar with.
       * You still have the scale problem in large range. It is better in the
         low range like in histogram but you do not have the choose bin size.
         Close to zero (left side of the vertical bar - mean value !?. The small horizontal bar is the std) you see more details,
         because there is no bin to select.

     #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-29 09:44:12
     [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-29_09-44-12.png]]
    
     * Plot 1 - CDF(1) + loglog scales
       * long tails distributions the probability to get extremely values are
         much large than the ones we have in normal distributions. Now we can
         see the differences in the interested range of latency - between micro
         seconds (10^{-15}) and milliseconds (10^{-3}).

     #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-29 09:50:12
     [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-29_09-50-12.png]]
     
     The results of experiment
     #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-29 09:55:49
     [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-29_09-55-49.png]]
     
     
       
     * sequential access because is much faster than random access
       and is highly affect by the size of block
     * write 10GB on disk sequentially takes about 8.9s, or less than 1 second
       per GB
     * write 1TB on disk at this rate would take ~1000 seconds or about 16
       minutes
       
       
     **Important results. SSD can be faster than memory RAM in sequential
       access**. 

     * The reason is because we are measuring *bandwidth* rather than latency.
     * We say it take 8.9 sec to write 10GB to SSD , we are NOT saying that
       write one byte it takes 8.9 x 10^{-10} second to write a single byte
     * This is because many write operations are occurring in parallel. Analogy
       is the bus. If you measure the time a people takes to go to A to B is
       long, but if you account all people inside the bus than this will be
       quite fast

     #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-29 10:01:10
     [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-29_10-01-10.png]]
     
     Comparison memory vs SSD
     
     | item | sequential access | random access  10KB | random access 10GB |
     | RAM  | 100MB/s           | 10^-9               | 10^-6 - 10^-3      |
     | SSD  | 1GB/s             | 10^-5 - 10^-3       | 10^-4 - 10^-1      |
     |------+-------------------+---------------------+--------------------|


     Summary of Macbook Pro hardware parameters

     #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-29 10:14:35
     [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-29_10-14-35.png]]
     
     * *The size of L3 cache is very important*
     * Latency has long have tails (And its is appears in many studies)
     * memory latency: 10^-9 - 10^-6
     * SSD 10^-5 - 10^-1


     Take away messages

     #+DOWNLOADED: /tmp/screenshot.png @ 2019-03-29 10:17:42
     [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-03-29_10-17-42.png]]
     
     **The important thing is avoid mover data around and around because of
       latency**. And its is hard to get used to it, because wwe are more custom to think in terms of CPU rate
*** Memory hierarchy

    * Hierarchy
      * small and fast storage close to CPU
      * large n slow further CPU
    * Caching
      * transfer data between levels of the hierarchy
    * Programmer point of view does NOT need to know that. hardware provide
      abstraction. memory looks like a single large array
    * BUT Performance depends on program's **access pattern**

    Example of memory hierarchy    (4 levels of storage)
    1. CPU
       * Register in the CPU
       * access time: ns
       * Block size: 32 Bytes
       * Size: 32 - 256KB
    2. L2 Cache
       * access time: between ns - ms
       * Block Size: 64B
       * Size: 1 - 12 MB
    3. Memory
       * access time: ns - ms
       * Block Size: 8KB
       * Size: 1 - 32 GB
    4. Disks
       * access time: ms
       * Block Size: 
       * Size: 100 GB - 1 PB

    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-02 10:29:31
    [[file:img/Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-02_10-29-31.png]]


    Cluster computers extend the hierarchy (analogy with one computer)
    * Cluster a group of computers 
    * Storage is shared
    * Locality: Data reside on the computer will use it
    * "Caching" is replaced by the world "Shuffling"
    * Abstractions  is spark RDD

   Sizes, latency and clusters
      * Size are spread 12 orders of magnitude
      * Latency: 6 orders of magnitude
      * Block size: 5 orders (less important)

    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-02 10:52:50
    [[file:img/Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-02_10-52-50.png]]

*** History of Big computation

    1. Commodity hardware: storage is cheap
    2. Locality: Data close to CPU
    3. Redundancy: Can recovery from server failures
    4. Simple abstraction: the developer point of views looks like a normal file
       system (chunk n redundancy mechanism are hidden)


    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-03 10:19:47
    [[file:img/Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-03_10-19-47.png]]


    Redundancy recovery
    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-03 10:26:25
    [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-03_10-26-25.png]]
    

    Locality

    * We want to process chunk in parallels as much as you can
    * Ex:
      * We do not want to process File 1 Chunk 1 and File 1 Chunk 2 in the same
        node because Chunk 2 will be only process after Chunk 1
      * I better solution is process File1 Chunk 2 in another note See the
        figure bellow
        
    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-03 10:29:59
    [[file:Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-03_10-29-59.png]]
    

    Map-Reduce 

    * HDFS is a storage abstraction
    * Map-reduce is a computation abstraction that works well with HDFS
    * Allows developers to specify parallel computation without knowing how the
      hardware is organized

    Spark

    * Developed by Matei Zaharia, amplab 2014
    * Hadoop uses shared file systems (disk)
    * Spark uses shared memory- faster n lower latency

*** Map Reduce and Spark
    
    Comparing with traditional way in python 
    
    * Operation over all elements of a list
    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-11 10:20:24
    [[file:img/Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-11_10-20-24.png]]


    * Summarizing or aggregations in a list (reduce)
    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-11 10:24:24
    [[file:img/Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-11_10-24-24.png]]

    
    * Map n reduce
    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-11 10:25:34
    [[file:img/Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-11_10-25-34.png]]

    Map reduce operations can not dependent on:
    * Order of the items in the list (commutativity)
    * Order of operators (Associativity)


    If you do in the wrong way (depends of the order of items or operations)
    , we will get a different results every time we run it because the
    cluster can execute the operations in different orders and ways

    Comparing executions 
    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-11 10:30:17
    [[file:img/Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-11_10-30-17.png]]


    Why order independence is important?

    * Computation order can be chosen by compiler/optimizer
    * Allows for parallel computation of sums of subsets
      * parallels computation is hard to coding
    * Map reduce programmer exposes to the compilers opportunities for
      parallel computations
*** Spark architecture

    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-12 10:12:16
    [[file:img/Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-12_10-12-16.png]]
    
    * Spark Driver (1st box)
      * runs on the master
      * execute the "main()" code program
    * Cluster Master (2nd Box)
      * manages computations resources. He knows where the data is and where
        the data copies are located
    * Workers
      * manage a single CORE in a node (normally in worker per core in a node)
      * RDD are partitioned among the workers. (Each work has a piece of the
        RDD)
      * manage partitions and Executors
      * Executors execute tasks on their partition, they are myopic. they do
        know nothing about the other workers and their data

    Spark context is abstraction that encapsulates the cluster for the
    driver

    * Stage are a group of transformation that does need to create a
      physical RDD. It is important for performance and spark takes care
      about it for you.
    * Stages end when RDD needs to be materialized. Created
    * Stages define the execution plan or physical plan

    #+DOWNLOADED: /tmp/screenshot.png @ 2019-04-12 10:24:02
    [[file:img/Big%20Data%20Analytics%20Using%20Spark/screenshot_2019-04-12_10-24-02.png]]

*** Manipulation plan RDD
    
    3 groups of commands
    1. Creation: RDD from files, databases, or data on the driver
    2. Transformations: transform RDD in another RDD
    3. Actions: RDD to data on driver node, databases or files

*** Manipulation Key values RDD

    iterators and python
    
    #+begin_src python
      # Waste memory because creates the list of elements
      for i in range(10000000000):
          #do something
          pass


      # No waste of memory
      for i in xrange(10000000000):
          #do do something
          pass

    #+end_src

    The *xrange* is equivalent in C
    #+begin_src c
      /*  No waste of memory */
      for(i=0; i<100000000000000; i++){

        /* do somethings */
       }
    #+end_src

    groupByKey return (key, <iterator>)

    #+begin_src python
      A=sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])

      A.groupByKey().map(lambda k, iter: (k,[x for x in iter ]))

      # output
      [ (1, [3,-5]), (3, [100, 2]) ]
    #+end_src

    1. countByKey: returns dictionnary

    #+begin_src python
      A=sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])

      A.countByKey()

      # output (dictionnary
      {1:2, 3:2}
    #+end_src

    2. lookup (key): returns the list of all of the values associated with
       key

    #+begin_src python
      A=sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])

      A.lookup(3)

      # output (dictionnary
      [100,2]
    #+end_src

    3. collectAsMap(): like collect() - collect returns list of tuples -  but returns a map = Dictionary

    #+begin_src python
      A=sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])

      A.collectAsMap()

      # output (dictionnary
      {1:[3,-5], 3: [100,2]}
    #+end_src



*** Spark Notebooks


    * RDD is the basic data structure. You can think as a list spread in
      many computers
    * collects is the opposites of parallelize. After a collect you bring to
      memory and you loose parallelism. collects return a list
    * Map
      * applies an operations to each element of an RDD
      * returns a RDD
      * operates in parallel
      * each executor operates on the data local to it

        #+begin_src python
          A.map(lambda x: x*x).collect()
        #+end_src



    * Reduce
      * input is RDD and return a single value
      * each executor reduce the data local to it
      * the results from all executors are then combined


       #+begin_src python
         words = ['this', 'is', 'the', 'best', 'mac', 'ever']

         wordRDD = sc.parallelize(words)

         wordRDD.reduce(lambda w,v: w if len(w) < len(v) else v)

         # another example is sum (the prder does not matter)
         B=sc.parallelize([1,3,5,2])

         B.reduce(lambda x,y: x+y)
       #+end_src

       If you do operation that depends of the order you can still do it but
       you will get different results


       #+begin_src python
         B=sc.parallelize([1,3,5,2])

         B.reduce(lambda x,y: x-y)
       #+end_src

       
       x - y is different of y - x

       * if the order is: ((1-3) - 5) - 2 = -9
       * if the oreder is: (1-3) - (5 - 2) = -5 



    
    
